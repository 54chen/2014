  <?xml version="1.0" encoding="utf-8"?>
  <feed xmlns="http://www.w3.org/2005/Atom">
    <title><![CDATA[Category: Java | 五四陈科学院]]></title>
    <link href="https://www.54chen.com/blog/categories/java/atom.xml" rel="self"/>
    <link href="https://www.54chen.com/"/>
    <updated>2017-12-29T18:10:29+08:00</updated>
    <id>https://www.54chen.com/</id>
    <author>
      <name><![CDATA[54chen]]></name>
    </author>
    <generator uri="http://octopress.org/">Octopress</generator>
  
    
    <entry>
      <title type="html"><![CDATA[Java IO在各版本中的提高]]></title>
      <link href="https://www.54chen.com/blog/2015/02/04/enhancements-in-java-io/"/>
      <updated>2015-02-04T15:22:29+08:00</updated>
      <id>https://www.54chen.com/blog/2015/02/04/enhancements-in-java-io</id>
      <content type="html"><![CDATA[<p><img src=http://www.oracle.com/technetwork/java/javaspotlight-189455.png></p>
  
  <h1>JDK8中的提高</h1>
  
  <p>在标准的（java.nio.charset.Charset）和扩展的charset实现方面，有许多改进。包括：</p>
  
  <ul>
  <li>SelectorProvider在Solaris上有新实现（基于Solaris事件端口机制）。这种SelectorProvider有可能在一些工作负荷下有可能会提升性能和扩展性。/dev/poll这种SelectorProvider依旧是默认的。要使用Solaris事件端口机制，启动的时候要设置系统属性java.nio.channels.spi.Selector为sun.nio.ch.EventPortSelectorProvider。</li>
  <li>将<JDK_HOME>/jre/lib/charsets.jar文件大小减小。</li>
  <li>java.lang.String(byte[], *)构造函数和java.lang.String.getBytes()方法的性能改善。</li>
  </ul>
  
  
  <!--more-->
  
  
  <h1>JDK7中的提高</h1>
  
  <p>java.nio.file包和相关包，java.nio.file.attribute，提供了对文件I/O和进入文件系统全面的支持。Zip文件系统在JDK7也可用了。下面的资源提供了更多信息。</p>
  
  <ul>
  <li>手册中加入File I/O (NIO 2.0特性)。NIO是指non-blocking I/O。</li>
  <li>可以开发自定义的File System Provider。</li>
  <li>可直接使用的Zip File System Provider。</li>
  <li><Java home>/sample/nio/chatserver/目录下有些演示java.nio.file包的例子。</li>
  <li><Java home>/demo/nio/zipfs/目录下有些演示NIO.2 NFS的例子。</li>
  </ul>
  
  
  <p>另外，下面的改进也被引进：</p>
  
  <ul>
  <li>JDK7之前，要开direct buffer得使用java.nio.ByteBuffer.allocateDirect，会对齐在一页的范围里。JDK中，实现被修改了，申请到的direct buffer将不再是页对齐的。这样的好处是减少了内存的浪费，但会创建不少小的buffer。</li>
  </ul>
  
  
  <h1>JDK6中的提高</h1>
  
  <h2>java.io</h2>
  
  <p>一个新的类：</p>
  
  <ul>
  <li>Console</li>
  </ul>
  
  
  <p>为File提供了新的方法：</p>
  
  <ul>
  <li>为了解磁盘信息准备的方法：</li>
  <li>getTotalSpace()</li>
  <li>getFreeSpace()</li>
  <li>getUsableSpace()</li>
  <li>为设置权限准备的方法：</li>
  <li>setWritable</li>
  <li>setReadable</li>
  <li>setExecutable</li>
  <li><p>canExecute</p></li>
  <li><p>为IOException类增加了新的构造方法 IOException(String, Throwable) 和 IOException(Throwable)。</p></li>
  <li><p>File.isFile在windows的实现有变化。</p></li>
  </ul>
  
  
  <h2>java.nio</h2>
  
  <ul>
  <li><p>基于Linux epoll事件通知机制，有了 java.nio.channels.SelectorProvider的新实现。epoll在Linux2.6及以上的内核中才有。当有成千上万的SelectableChannels注册在一个Selector上时，基于epoll实现的SelectProvider比传统的能更可扩展。新的SelectorProvider实现会在2.6内核上默认使用。2.6以下的内核默认还是原来基于poll的实现。</p></li>
  <li><p>sun.nio.ch.disableSystemWideOverlappingFileLockCheck系统属性，设置了FileChannel要给一个文件加锁的时候，能不能多次加。</p></li>
  </ul>
  
  
  <h1>JDK5.0中的提高</h1>
  
  <h2>java.nio</h2>
  
  <ul>
  <li>新加javax.net.ssl.SSLEngine。之前只能用SSLSocket自己搞。</li>
  </ul>
  
  
  <h1>J2SDK1.4中的提高</h1>
  
  <h2>java.nio</h2>
  
  <ul>
  <li>在FileInputStream和FileOutputStream中添加了getChannel方法，返回FileChannel，添加了close方法。</li>
  <li>RadomAccessFile中也同上。</li>
  <li>为InputStreamReader和OutputstreamWriter类添加构造函数可传入Charset对象。添加了getEncoding方法。</li>
  </ul>
  
  
  <h2>java.nio</h2>
  
  <ul>
  <li>nio包被添加。</li>
  <li>JNI支持direct buffer添加了三个新方法。</li>
  </ul>
  
  
  <h1>过去的提高</h1>
  
  <ul>
  <li>java.io.File</li>
  <li>java.io包</li>
  <li>java.io charset相关</li>
  </ul>
  
  ]]></content>
    </entry>
    
    <entry>
      <title type="html"><![CDATA[小米互娱招java工程师]]></title>
      <link href="https://www.54chen.com/blog/2014/08/25/xiao-mi-hu-yu-zhao-javagong-cheng-shi/"/>
      <updated>2014-08-25T21:17:53+08:00</updated>
      <id>https://www.54chen.com/blog/2014/08/25/xiao-mi-hu-yu-zhao-javagong-cheng-shi</id>
      <content type="html"><![CDATA[<p><img src=http://img.taobaocdn.com/imgextra/i4/13078490/T2UAulXnlbXXXXXXXX_!!13078490.png></p>
  
  <h2>序</h2>
  
  <ul>
  <li>这是一个新的时代。</li>
  <li>小米互娱作为一个真正意义上的小米互联网，开启小米“铁人三项”中互联网征途。</li>
  <li>这里有众多靠谱的兄弟。</li>
  <li>这里缺更加强大的你的加入。</li>
  </ul>
  
  
  <!--more-->
  
  
  <h2>寻</h2>
  
  <ul>
  <li>java工程师 1－5年工作经验。</li>
  </ul>
  
  
  <h2>项目</h2>
  
  <ul>
  <li>你可能需要理解Big-endian、Little-endian这种细节。</li>
  <li>你可能需要掌握amqp协议细节。</li>
  <li>你可能需要了解移动网络下出现的异常。</li>
  <li>你可能需要清楚http CONNECT方法的作用。</li>
  <li>你可能需要明白restful的道理。</li>
  <li>你可能应该更加主动地联系54chen at xiaomi dot com。</li>
  </ul>
  
  
  <h2>待遇</h2>
  
  <ul>
  <li>过了技术关，剩下的事情全部和老板直接谈，请畅所欲言。</li>
  <li>欲大展拳脚的同学，是不错的机会。</li>
  <li>抽空过来聊一聊，多次机会多条路。</li>
  </ul>
  
  
  <p> 一些实在的码，麻烦看到的兄弟帮我转：</p>
  
  <ul>
  <li>红米手机1S 电信版 8GB 金属灰HMDX96763527807262227 8.1前</li>
  <li>红米手机1S 移动版 8GB 金属灰HMSY86526398236552885 8.1前</li>
  <li>小米电视2                   MTV240078896615240317 8.1前</li>
  <li>小米平板 16GB 纯白色MPAD59271074707309411         8.1前</li>
  <li>欲要更多，请来简历54chen at xiaomi dot com。</li>
  </ul>
  
  ]]></content>
    </entry>
    
    <entry>
      <title type="html"><![CDATA[线上JVM调查工具:JCPU和JMEM]]></title>
      <link href="https://www.54chen.com/blog/2014/05/07/jvm-cpu-mem-tools/"/>
      <updated>2014-05-07T17:05:13+08:00</updated>
      <id>https://www.54chen.com/blog/2014/05/07/jvm-cpu-mem-tools</id>
      <content type="html"><![CDATA[<p><img src=http://www.oracle.com/technetwork/java/javaspotlight-189455.png></p>
  
  <p>之前的jkiller改名为jcpu，然后再加上jmem，齐活了，线上要再遇到问题，内存和CPU之外的也没啥其他办法了。</p>
  
  <h2>JMEM</h2>
  
  <p><a href="https://github.com/54chen/jmem">https://github.com/54chen/jmem</a></p>
  
  <p>用来定位莫名其妙的堆外内存问题。首先还是要先用jmap之类的看清楚是否是JVM堆内问题了再用此神物。</p>
  
  <!--more-->
  
  
  <p> jmem.sh 用来靠gdb找到够大的内存块，直接dump到文件里。然后肉眼看吧。。。反正我没看出来，祝你好运。</p>
  
  <p> pmap2stack.sh 弄出来大块的内存地址后，尝试在各种stack中帮你grep出来可以读东东。</p>
  
  <h2>JCPU</h2>
  
  <p><a href="https://github.com/54chen/jcpu">https://github.com/54chen/jcpu</a></p>
  
  <p>这个之前有介绍过，就是cpu占得比较猛的进程，直接打出来里面最费CPU的前五个堆栈。</p>
  
  <p>具体用法都在项目的README上。</p>
  ]]></content>
    </entry>
    
    <entry>
      <title type="html"><![CDATA[Disruptor Thrift Server连接参数与rps数值影响记录]]></title>
      <link href="https://www.54chen.com/blog/2014/01/22/disruptor-thrift-server-link-param-rps/"/>
      <updated>2014-01-22T00:00:00+08:00</updated>
      <id>https://www.54chen.com/blog/2014/01/22/disruptor-thrift-server-link-param-rps</id>
      <content type="html"><![CDATA[<p><img src="http://chen54.b0.upaiyun.com/0122/267176d6-5ef4-3e0b-937e-c492f5ba7807.jpg" alt="disruptor thrift server" width="450/" /></p>
  
  
  
  
  <p><strong>基础环境</strong>
  rmbp<br />
  8G MEM<br />
  Intel(R) Core(TM) i7-3615QM CPU @ 2.30GHz  四核 八线程<br />
  oracle jdk 1.7.0_45</p>
  
  
  
  
  <p><strong>原理</strong>
  如上图，是类似tomcat的nio实现过程，不过将queue换成了高性能的disruptor，可能会得到更好的性能。可调整参数为numAcceptors，numSelectors，numWorkersPerSelector三个值。<br />
  测试的代码位置在 https://github.com/54chen/disruptor_thrift_server 项目中的BenchmarkTest同时启动一个server，同时对其进行压测。</p>
  
  
  <!-- more -->
  
  
  <p>AbstractDisruptorTest.Service类中实现了server的逻辑，里面实际上是一个加减乘除的简单方法，故意加了300ms的sleep以尽可能模拟真实的消费时间。<br />
  BenchmarkTest类中实现了client的压力逻辑，固定了200个线程同时死循环地进行访问，同时进行计数统计rps。</p>
  
  
  
  
  <p><strong>调整记录</strong>
  1.A(cceptors):4  S(electors):2 W(orkersPerSelector):2</p>
  
  
  
  
  <p>处理接入的线程数:4  <br />
  处理调用逻辑的线程数:2 推荐是cpu的核数以上<br />
  每个处理逻辑上的worker数量:2</p>
  
  
  
  
  <p>结果：<br />
  Rate: 16 req/s<br />
  Rate: 12 req/s<br />
  Rate: 12 req/s</p>
  
  
  
  
  <p>无reset等异常。</p>
  
  
  
  
  <p>2.A:40  S:2  W:2</p>
  
  
  
  
  <p>结果：<br />
  Rate: 16 req/s<br />
  Rate: 12 req/s<br />
  Rate: 12 req/s</p>
  
  
  
  
  <p>4.A:40  S:8 W:2<br />
  Rate: 48 req/s<br />
  cpu:10%  线程数:287</p>
  
  
  
  
  <p>5.A:40  S:80 W:2<br />
  Rate: 385 req/s<br />
  cpu:25%  线程数:503</p>
  
  
  
  
  <p>6.A:40 S:100 W:2<br />
  Rate: 400-500 req/s<br />
  cpu:30%  线程数:563<br />
  开始出现reset异常 java.net.SocketException: Connection reset</p>
  
  
  
  
  <p>7.A:40 S:100 W:4<br />
  Rate: 600-700 req/s<br />
  cpu:50%  线程数:763<br />
  开始出现java.net.NoRouteToHostException: Can't assign requested address<br />
  极其不稳定，还会出现too many open files的异常<br />
  这是因为客户端同服务器端在同一机器的原因，受到了ulimit之类的限制<br />
  还出现了java.net.ConnectException: Operation timed out</p>
  
  
  
  
  <p><strong>后记</strong>
  上面的尝试都是在我的笔记本上进行的，因为mac不像centos这么好调整ulimit maxsockets这些参数，所以点到即止。<br />
  简单说，A值是水龙头，S值是分水管道，W值是发电机，W的值与CPU的负载有直接关系，所以一般以CPU的核数来对这个值进行调优。</p>
  
  
  
  
  <p></p>
  
  ]]></content>
    </entry>
    
    <entry>
      <title type="html"><![CDATA[Cassandra的thrift用法学习手记]]></title>
      <link href="https://www.54chen.com/blog/2013/12/30/cassandra-thrift/"/>
      <updated>2013-12-30T00:00:00+08:00</updated>
      <id>https://www.54chen.com/blog/2013/12/30/cassandra-thrift</id>
      <content type="html"><![CDATA[<p><img src="http://img04.taobaocdn.com/imgextra/i4/T1gzmqXXdpXXcwYVo0_034121.jpg" alt="thrift" />
  <a href="#en">__English Version__The notes about the usage of Thrift in Cassandra</a></p>
  
  
  
  
  <p>Cassandra在client访问server cluster的时候使用了thrift，在cluster node间的通讯，依旧是自己实现的二进制协议。</p>
  
  
  
  
  <p><strong>先决条件</strong>
  thrift 0.9.1<br />
  mac<br />
  libthrift-0.9.1<br />
  Cassandra 2.0.3</p>
  
  
  
  
  <p>本文所涉及的代码：<a href="https://github.com/54chen/cassandra-thrift">https://github.com/54chen/cassandra-thrift</a></p>
  
  
  <!-- more -->
  
  
  <p><strong>看代码</strong>
  thrift的定义：server的逻辑实现代码叫做Processor，创建的等待socket代码里叫做Transport，最后启动的进程叫做Server，大致就会有要启动一个thrift server，需要有一个socket（Transport）和一堆逻辑（Processor）。</p>
  
  
  
  
  <p>这里有一篇2011年写的thrift入门手记：<a href="http://www.54chen.com/java-ee/thrift-quick-start.html">http://www.54chen.com/java-ee/thrift-quick-start.html</a></p>
  
  
  
  
  <p>示例代码中以TNonBlockingServer来举例。TNonBlockingServer是采用libevent lib实现的一种thrift内置的server类型，理论上是最快的一种。</p>
  
  
  
  
  <p><strong>CustomTNonBlockingServer</strong>
  Cassandra并没有默认使用这种类型的server，默认的是基于TServerTransport连接完成的CustomTThreadPoolServer（自行实现了连接池）。</p>
  
  
  
  
  <p>以示例代码为例：<br />
  CassandraTestServer －> CustomTNonBlockingServer.Factory().buildTServer -> new CustomTNonBlockingServer -> new TCustomNonblockingServerSocket.</p>
  
  
  
  
  <p>这个调用传递中，在接入客户端连接时，可以插入一定的代码（比如说来来保存session之类的）。</p>
  
  
  
  
  <p>代码中被插入的代码和阶段有：<br />
  TCustomNonblockingServerSocket->acceptImpl  一次客户端请求进入时，可用来设置服务器端的超时设置之类的数值。<br />
  CustomTNonBlockingServer->requestInvoke   紧接着上面的执行后具体的调用会打到这里来，这里看上去Cassandra把一个父类强转为子类，为了取当时的socket信息，有点小bug。</p>
  
  
  
  
  <p><strong>其他server</strong>
  Cassandra默认的CustomTThreadPoolServer也有类似前文提到的插入代码的地方，主要目的也是超时设置buffer设置等，而且因为使用的是普通TServerTransport，设置的点更加灵活。<br />
  不得不提的是另一个的实现，THsHaDisruptorServer，大有来头。<br />
  THsHaDisruptorServer使用TCustomNonblockingServerSocket当socket，上层TDisruptorServer使用的是基于LMAX（一家牛B的金融平台公司）开源的Disruptor（一个巨牛B的高吞吐低延迟的并发处理机制）来实现的TDisruptorServer，TDisruptorServer的代码在：<a href="https://github.com/xedin/disruptor_thrift_server">https://github.com/xedin/disruptor_thrift_server</a> 可以理解为一个经过队列改造的TNonBlockingServer。</p>
  
  
  
  
  <p>以后再找机会分析这个disruptor_thrift_server。<br />
  __EOF__</p>
  
  
  
  
  <p><a name="en">__English Version__</a>
  Cassandra uses thrift when the client visits the server, but it still uses binary protocol between the cluster nodes. </p>
  
  
  
  
  <p><strong>Prerequisite</strong>
  thrift 0.9.1<br />
  mac<br />
  libthrift-0.9.1<br />
  Cassandra 2.0.3</p>
  
  
  
  
  <p>The codes mentioned in this blog：<a href="https://github.com/54chen/cassandra-thrift">https://github.com/54chen/cassandra-thrift</a></p>
  
  
  
  
  <p><strong>See Codes</strong>
  The definition of Thrift: The codes which implements the server is named Processor. The codes which implements the socket for waiting is named Transport. At last, the codes which starts all logic is named Server. All of this, we need a thrift server,a socket(Transport) and a Processor.</p>
  
  
  
  
  <p>This is a blog link about thrift written in 2011:<a href="http://www.54chen.com/java-ee/thrift-quick-start.html">http://www.54chen.com/java-ee/thrift-quick-start.html</a></p>
  
  
  
  
  <p>The example codes take TNonBlockingServer for example.TNonBlockingServer is a built-in type server designed by lib-event.It is the fast type  in theory.</p>
  
  
  
  
  <p><strong>CustomTNonBlockingServer</strong>
  Cassandra does not use this type by default.The default type is based on TServerTransport named CustomTThreadPoolServer(implements the thread pool).</p>
  
  
  
  
  <p>For example:<br />
  CassandraTestServer －> CustomTNonBlockingServer.Factory().buildTServer -> new CustomTNonBlockingServer -> new TCustomNonblockingServerSocket.</p>
  
  
  
  
  <p>In this link,when client is come,we can plug in some codes(for saving session, etc.）。</p>
  
  
  
  
  <p>the codes and the stages which plugged in:<br />
  TCustomNonblockingServerSocket->acceptImpl  When the client is comming,we can set the configuration like the value of timeout,etc.<br />
  CustomTNonBlockingServer->requestInvoke   And then it will be called here.It seems that Cassandra's codes cast a parent class to children one.It is maybe a bug。</p>
  
  
  
  
  <p><strong>Else Server</strong>
  Cassandra's default server CustomTThreadPoolServer is also can be plugged in like above talked.The plugin main target is set timeout,buffer,etc.And the TServerTransport it used is more flexible to setting values。</p>
  
  
  
  
  <p>It's hard to ignore the another type,THsHaDisruptorServer,is not normaI。<br />
  THsHaDisruptorServer is use TCustomNonblockingServerSocket to socket.The TDisruptorServer is based on LMAX Disruptor.The TDisruptorServer's codes is here:<a href="https://github.com/xedin/disruptor_thrift_server">https://github.com/xedin/disruptor_thrift_server</a> Can be understood as,a queue-improved TNonBlockingServer。</p>
  
  
  
  
  <p>I'll write a blog about disruptor_thrift_server later.<br />
  __EOF__</p>
  
  
  
  
  <p>Happy New Year!</p>
  
  ]]></content>
    </entry>
    
  </feed>
